{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "deletable": false,
        "pbl_cell_type": "hidden_setup_code",
        "step_id": 4497,
        "step_number": 0,
        "trusted": false
      },
      "outputs": [],
      "source": "#hiddencell\nfrom pbl_tools import *\n\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfe = fm.FontEntry(fname = 'NotoSansKR-Regular.otf', name = 'NotoSansKR')\nfm.fontManager.ttflist.insert(0, fe)\nplt.rc('font', family='NotoSansKR')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "pbl_cell_type": "markdown",
        "step_id": 4497,
        "step_number": 0,
        "tags": []
      },
      "source": "# 스테이지 6. test 데이터 전처리 및 모델 학습과 평가\n\n## 도입 \n\n이번 스테이지에서는 'test 데이터 전처리 및 모델 학습'에 중점을 두고 진행됩니다. \n이전 스테이지에서는 데이터의 기본적인 구조와 통계량을 확인하였으며,   \n이제는 테스트 데이터를 다루며 데이터 전처리와 모델 학습을 통해 예측 모델을 고도화하고자 합니다.\n\n## 학습 목표  \n- 테스트 데이터에 대한 전처리를 수행하고 필요한 새로운 특성을 생성할 수 있다.\n- 독립 변수와 종속 변수를 명확히 분리할 수 있다.\n- PCA (주성분 분석) 모델을 활용하여 데이터의 차원을 축소하고 주요 특성을 추출할 수 있다.\n- GradientBoostingClassifier 모델을 활용하여 테스트 데이터를 분석하고 모델 학습을 진행할 수 있다."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4498,
        "step_number": 1
      },
      "source": "# 1. 데이터 전처리 및 새로운 특성 생성\n[문제 1]  \n- education 피처의 `Some-college, Assoc-acdm, Assoc-voc` 값들을 `SomeHigherEd`로 재구분해주세요. 이를 원래의 데이터프레임에 적용될 수 있도록 해주세요.\n- marital.status 피처의 `'Separated', 'Divorced'` 값들을 `MarriageEnded` 로 재구분해주세요. 이를 원래의 데이터프레임에 적용될 수 있도록 해주세요.\n- `age` 피처와 `hours.per.week` 피처를 곱한 `age-hours` 파생변수를 생성해 주세요.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4498,
        "step_number": 1,
        "trusted": false
      },
      "outputs": [],
      "source": "import pandas as pd  \n\ntrain = pd.read_csv('train.csv')  \ntest = pd.read_csv('test.csv')  \n\n# 교육(education) 수준 카테고리 재분류\ntrain['education'].replace(['Preschool', '10th', '11th', '12th', '1st-4th', '5th-6th', '7th-8th', '9th'], 'LowEducation', inplace=True)\ntrain['education'].replace([___, ___, ___], ___, inplace=___)\ntrain['education'].replace(['Masters', 'Prof-school'], ['Masters', 'Masters'], inplace=True)\n\n#결혼 상태 (marital.status) 수준 카테고리 재분류\ntrain['marital.status'].replace(['Never.married', 'Married.spouse.absent'], 'UnmarriedStatus', inplace=True)\ntrain['marital.status'].replace(['Married.AF.spouse', 'Married.civ.spouse'], 'Married', inplace=True)\ntrain['marital.status'].replace(...)\n\n# 나이(age)와 주당 근무시간(hours.per.week)을 곱한 새로운 특성 생성\ntrain['age-hours'] = train[___] * train[___]"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4498,
        "step_number": 1,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nensure_vals(globals(), 'train')\n@check_safety\ndef check(\n    df: pd.DataFrame,\n    col: str,\n    val1: str,\n    val2: str,\n    val3: str,\n    new_col: str\n):\n    c_point0 = hasattr(df, 'head')\n    c_point1 = val1 in df[col].unique()\n    c_point2 = val2 in df[col].unique()\n    c_point3 = val3 not in df[col].unique()\n    c_point4 = new_col in df.columns\n\n    if (\n        c_point0 and \n        c_point1 and \n        c_point2 and \n        c_point3 and \n        c_point4\n    ):\n        return True\n    else:\n        return False\n\ncheck(train, 'education', 'LowEducation', 'SomeHigherEd', 'Prof-school', 'age-hours')"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4499,
        "step_number": 2
      },
      "source": "# 2.test 데이터에 대한 전처리 및 새로운 특성 생성\n\ntrain 데이터셋에 적용한 데이터 전처리를 test 데이터셋에도 동일하게 적용하여 일관성을 유지하고 모델의 일반화 성능을 향상시킵니다."
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4499,
        "step_number": 2,
        "trusted": false
      },
      "outputs": [],
      "source": "# 교육(education) 수준 카테고리 재분류\ntest['education'].replace(['Preschool', '10th', '11th', '12th', '1st-4th', '5th-6th', '7th-8th', '9th'], 'LowEducation', inplace=True)\ntest['education'].replace(['Some-college', 'Assoc-acdm', 'Assoc-voc'], 'SomeHigherEd', inplace=True)\ntest['education'].replace(['Masters', 'Prof-school'], ['Masters', 'Masters'], inplace=True)\n\n#결혼 상태 (marital.status) 수준 카테고리 재분류\ntest['marital.status'].replace(['Never.married', 'Married.spouse.absent'], 'UnmarriedStatus', inplace=True)\ntest['marital.status'].replace(['Married.AF.spouse', 'Married.civ.spouse'], 'Married', inplace=True)\ntest['marital.status'].replace(['Separated', 'Divorced'], 'MarriageEnded', inplace=True)\n\n# 나이(age)와 주당 근무시간(hours.per.week)을 곱한 새로운 특성 생성\ntest['age-hours'] = test['age']*test['hours.per.week']"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4500,
        "step_number": 3
      },
      "source": "# 3.결측치 처리와 불필요한 열 제거\n\n[문제 2]\n- 결측치 처리를 위한 `SimpleImputer` 클래스를 가져오세요.    \n- `SimpleImputer` 객체를 생성하고, 결측치를 처리할 때 해당 특성에서 가장 자주 발생하는 값으로 결측치를 대체해주세요.\n- 학습 데이터셋인 `train`의 'occupation'과 'workclass' 열에 대해 결측치를 `최빈값`으로 채워주세요.\n- 테스트 데이터셋인 `test`의 'occupation'과 'workclass' 열에 대해 결측치를 `최빈값`으로 채워주세요.\n- train, test 데이터셋에서 불필요한 피처인 `'native.country','ID'` 을 제거해 주세요."
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4500,
        "step_number": 3,
        "trusted": false
      },
      "outputs": [],
      "source": "from sklearn.impute import ___\n\n# SimpleImputer를 사용하여 결측치를 최빈값으로 보간\nimputer = ___(strategy='___')\ntrain[['occupation','workclass']] = ___.___(...)\ntest[['occupation','workclass']] = ___.___(...)\n\n# 불필요한 열 제거\ntrain = train.___(...)\ntest = test.___(...)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4500,
        "step_number": 3,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nensure_vals(globals(), 'x_train', 'imputer')\n@check_safety\ndef check(\n    df: pd.DataFrame,\n    encoder: SimpleImputer,\n    not_col: str,\n    use_col1: str,\n    use_col2: str\n):\n    c_point0 = not_col not in df.columns\n    c_point1 = use_col1 in encoder.feature_names_in_\n    c_point1 = use_col2 in encoder.feature_names_in_\n\n    if c_point0 and c_point1:\n        return True\n    else:\n        return False\n\ncheck(train, imputer, 'native.country', 'workclass', 'occupation')"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4501,
        "step_number": 4
      },
      "source": "# 4.원-핫 인코딩을 사용한 범주형 변수 변환\n\n[문제 3]\n- scikit-learn의 preprocessing 모듈에서 `OneHotEncoder` 클래스를 불러오세요.\n- `OneHotEncoder` 객체를 생성합니다. \n- 학습 데이터셋(`train`)의 `'race', 'sex', 'marital.status'` 열을 선택하고, 원-핫 인코딩을 적용해주세요.\n- 테스트 데이터셋(`test`)의 `'race', 'sex', 'marital.status'` 열을 선택하고, 원-핫 인코딩을 적용해주세요.\n- 원-핫 인코딩된 훈련 데이터를 새로운 데이터프레임으로 변환해주세요.   \n컬럼 이름은 'race_xxx', 'sex_xxx', 'marital.status_xxx' 형태의 이름으로 생성해주세요. 여기서 xxx는 해당 카테고리 변수의 각 범주를 나타냅니다.\n- test 데이터셋 기존의 인덱스를 삭제하고 새로운 연속적인 정수 인덱스로 설정해주세요.\n- test와 test_ohe를 열 방향(axis=1)으로 합쳐주세요."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4501,
        "step_number": 4,
        "trusted": false
      },
      "outputs": [],
      "source": "from sklearn.preprocessing import ___\n\nencoder = ___(sparse_output=False)\ntrain_encoded = ___.fit_transform(train[['race', 'sex', 'marital.status']])\ntest_encoded = ___.___(...)\n\n# 원-핫 인코딩된 데이터를 DataFrame으로 변환\ntrain_ohe = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out(['race', 'sex', 'marital.status']))\ntest_ohe = pd.DataFrame(___, columns=___.___(...))\n\n# 데이터프레임에 원-핫 인코딩된 특성 추가\ntrain = train.reset_index(drop=True)\ntest = test.___(___=___)\n\ntrain = pd.concat([train,train_ohe], axis=1)\ntest = pd.concat(...)"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4501,
        "step_number": 4,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nensure_vals(globals(), 'train', 'test')\n@check_safety\ndef check(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    enc: OneHotEncoder,\n    enc_name: str,\n    num_enc_feature: int,\n    len_train_col: int,\n    len_test_col: int\n):\n    \n    c_point0 = enc_name in str(enc)\n    c_point1 = len(enc.feature_names_in_) == num_enc_feature\n    c_point2 = len(train_df.columns) == len_train_col\n    c_point3 = len(test_df.columns) == len_test_col\n    \n\n    if (\n        c_point0 and \n        c_point1 and \n        c_point2 and \n        c_point3\n    ):\n        return True\n    else:\n        return False\n\ncheck(train, test, encoder, 'OneHotEncoder', 3, 28, 27)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4502,
        "step_number": 5
      },
      "source": "# 5.라벨 인코딩을 사용한 범주형 변수 변환\n\n[문제 4]   \n- `LabelEncoder` 객체를 생성하여 le 변수에 할당해 주세요.\n- train 의 현재 열의 타입이 object 인 경우 아래 셀을 실행해 주세요.\n- `train` 데이터프레임의 현재 열에 LabelEncoder을 적용해 주세요.\n- 만약 검증 데이터에서 새롭게 나타나는 범주값(label)이라면, 이를 인코더의 클래스 목록(le.classes_)에 추가해 주세요.\n- `test` 데이터프레임의 현재 열에 LabelEncoder을 적용해 주세요."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4502,
        "step_number": 5,
        "trusted": false
      },
      "outputs": [],
      "source": "from sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\nfor col in train.columns:\n    if train[col].___ == '___':\n        \n        le = ___()\n        train[col] = ___.___(___[___])\n\n        for label in np.unique(test[col]):\n            if label not in le.classes_:\n                le.classes_ = np.___(le.classes_, ___)\n        test[col] = ___.___(___[___])"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4502,
        "step_number": 5,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nensure_vals(globals(), 'train', 'test')\n@check_safety\ndef check(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    obj: str,\n    zero: 0,\n    enc_name: str\n):\n    \n    len_obj_train = len(train_df.select_dtypes(include=obj).columns)\n    len_obj_test = len(test_df.select_dtypes(include=obj).columns)\n    \n    c_point0 = len_obj_train == zero\n    c_point1 = len_obj_test == zero\n    c_point2 = enc_name in str(le)\n\n    if c_point0:\n        return True\n    else:\n        return False\n\ncheck(train, test, 'object', 0, 'LabelEncoder')"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4503,
        "step_number": 6
      },
      "source": "# 6.StandardScaler를 이용한 데이터 표준화\n\n[문제 5]\n\n- StandardScaler() 객체를 생성해 주세요.\n- train 데이터셋에 해당 피처 부분에 표준화를 적용해주세요.\n- test 데이터셋에 해당 피처 부분에 표준화를 적용해주세요.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4503,
        "step_number": 6,
        "trusted": false
      },
      "outputs": [],
      "source": "from sklearn.preprocessing import StandardScaler\n\nfeatures = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss','hours.per.week', 'age-hours']\n\nscaler = ___()\ntrain[features] = ...\ntest[features] = ..."
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4503,
        "step_number": 6,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nensure_vals(globals(), 'train', 'test', 'scaler')\n@check_safety\ndef check(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    scaler_class: StandardScaler,\n    num_features: int,\n    col1: str,\n    col2: str\n):\n    \n    c_point0 = len(scaler_class.feature_names_in_) == num_features\n    c_point1 = train_df[col1].dtypes != int\n    c_point2 = test_df[col2].dtypes != int\n\n    if c_point0 and c_point1 and c_point2:\n        return True\n    else:\n        return False\n\ncheck(train, test, scaler, 7, 'age', 'education.num')"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4504,
        "step_number": 7
      },
      "source": "# 7.독립 변수와 종속 변수 분리"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4504,
        "step_number": 7,
        "trusted": false
      },
      "outputs": [],
      "source": "train_x = train.drop(['target'],axis = 1)\ntrain_y = train['target']"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4505,
        "step_number": 8
      },
      "source": "# 8.데이터 차원 축소를 위한 PCA 모델 학습\n\n[문제 6]  \n- train_x 에 대해 PCA 모델을 학습시켜보세요. \n- `explained_variance_ratio_` 값을 누적해서 더해주세요.\n- 해당 임계값을 넘는 최초의 인덱스를 추출해 보세요.\n- 주성분 분석(PCA) 모델을 초기화해주세요. 주성분 개수를 전달해 주세요.\n- 초기화된 PCA 모델을 기존의 train_x 데이터에 다시 적용해주세요.테스트 데이터인 `test`에 동일한 PCA 모델을 적용합니다."
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4505,
        "step_number": 8,
        "trusted": false
      },
      "outputs": [],
      "source": "from sklearn.decomposition import PCA\n\n# PCA 모델 학습\npca = PCA()\npca.___(___)\n\n# 분산의 설명량 확인\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# 누적 설명량과 주성분 개수 선택\ncumulative_explained_variance = ___.___(explained_variance_ratio)\nn_components = ___.___(___ >= 0.95) + 1\n\n# PCA 모델 재설정 및 데이터 변환\npca = ___(n_components=___)\n\ntrain_x = pca.___(train_x)\ntest = pca.___(test)"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4505,
        "step_number": 8,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nimport numpy as np\nensure_vals(globals(), 'train_x', 'test','pca')\n@check_safety\ndef check(\n    train_array: np.array,\n    test_array: np.array,\n    pca_class: PCA,\n    indexing: int\n):\n    \n    c_point0 = len(pca_class.get_feature_names_out()) == train_array.shape[1]\n    c_point1 = len(pca_class.get_feature_names_out()) == test_array.shape[1]\n    \n    if c_point0 and c_point1:\n        return True\n    else:\n        return False\n\ncheck(train_x, test, pca, 1)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4506,
        "step_number": 9
      },
      "source": "# 9.GradientBoostingClassifier 모델을 활용한 데이터 분석 및 성능 평가\n\n\n[문제 7]\n\n- GradientBoostingClassifier 모델을 생성해 주세요. \n- 학습 데이터를 사용하여 학습시켜주세요.\n- 학습된 모델을 사용하여 테스트 데이터에 대한 예측을 수행합니다."
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4506,
        "step_number": 9,
        "trusted": false
      },
      "outputs": [],
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import f1_score\n\ngb_classifier = ___(random_state=30, \n                    max_depth=6,\n                    min_samples_split=3,\n                    max_features=10)\n\ngb_classifier.___(___, ___)\n\ny_pred = gb_classifier.___(___)"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4506,
        "step_number": 9,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nimport numpy as np\nensure_vals(globals(), 'gb_classifier', '')\n@check_safety\ndef check(\n    model_name: str,\n    model: GradientBoostingClassifier,\n    train_array: np.ndarray,\n    one: int,\n    pred_value: np.ndarray,\n    pred_shape: tuple\n):\n    \n    c_point0 = model_name in str(model)\n    c_point1 = model.n_features_in_ == (train_array.shape[one])\n    c_point2 = pred_value.shape == pred_shape\n    \n    if c_point0 and c_point1 and c_point2:\n        return True\n    else:\n        return False\n\ncheck('GradientBoostingClassifier', gb_classifier, train_x, 1, y_pred,(494,))"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4507,
        "step_number": 10
      },
      "source": "# 10.데이터프레임(DataFrame)을 CSV 파일로 저장하기"
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "pbl_cell_type": "submission_code",
        "pbl_submission_filename": "submission.csv",
        "step_id": 4507,
        "step_number": 10,
        "trusted": false
      },
      "outputs": [],
      "source": "submission = pd.read_csv('sample_submission.csv')\n\nsubmission['target'] = y_pred\nsubmission.to_csv('submission.csv', index=False)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}