{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "deletable": false,
        "pbl_cell_type": "hidden_setup_code",
        "step_id": 4479,
        "step_number": 0,
        "trusted": false
      },
      "outputs": [],
      "source": "#hiddencell\nfrom pbl_tools import *\n\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfe = fm.FontEntry(fname = 'NotoSansKR-Regular.otf', name = 'NotoSansKR')\nfm.fontManager.ttflist.insert(0, fe)\nplt.rc('font', family='NotoSansKR')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "pbl_cell_type": "markdown",
        "step_id": 4479,
        "step_number": 0
      },
      "source": "# 스테이지 5. 고급 데이터 전처리 및 차원 축소를 통한 모델 학습과 평가\n## 도입\n이번 스테이지에서는 고급 데이터 전처리와 차원 축소를 통한 모델 학습과 평가에 집중합니다.   \n이전 스테이지에서는 데이터의 기본 구조와 피처 상호 작용을 다뤘으며,   \n이번에는 더 깊은 수준의 데이터 전처리와 차원 축소 기술을 활용하여 모델을 개선하고자 합니다.\n\n## 학습 목표\n- 교육 수준(education)과 결혼 상태(marital.status)를 수준 카테고리로 재분류할 수 있다.\n- 파생변수를 생성하여 데이터를 보다 의미 있게 표현할 수 있다.\n- 범주형 변수를 원-핫 인코딩하여 모델 학습에 활용할 수 있다.\n- PCA (주성분 분석)를 활용하여 데이터의 차원을 축소하고 주요 특성을 추출할 수 있다.\n- PCA 주성분의 분산 설명량을 확인하고 누적 설명량을 고려하여 주성분 개수를 선택할 수 있다.\n- 데이터를 표준화하고 PCA 모델을 적용하여 데이터를 변환할 수 있다.\n- RandomForestClassifier 모델을 활용하여 데이터를 분석하고 성능을 평가할 수 있다."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4480,
        "step_number": 1
      },
      "source": "# 1. pandas를 이용해 csv 파일 읽어오기\n[문제 1]  \n`Pandas` 라이브러리(library)를 가져와보세요.  \n그리고 `train.csv`, `test.csv`, `sample_submission.csv` 파일을 각각 train, test, submission 변수로 읽어오세요.  \n아래 빈칸을 채워주세요.  "
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4480,
        "step_number": 1,
        "trusted": false
      },
      "outputs": [],
      "source": "import pandas as pd  \n\ntrain = pd.___('train.csv')  \ntest = pd.___('test.csv')  \nsubmission = pd.___('sample_submission.csv')"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4480,
        "step_number": 1,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nensure_vals(globals(), 'train', 'test', 'submission')\n@check_safety\ndef check(\n    user_train: pd.DataFrame,\n    user_test: pd.DataFrame,\n    user_submission: pd.DataFrame\n):\n    c_point1 = hasattr(user_train, 'tail')\n    c_point2 = hasattr(user_test, 'tail')\n    c_point3 = hasattr(user_submission, 'tail')\n\n    if c_point1 and c_point2 and c_point3:\n        return True\n    else:\n        return False\n\ncheck(train, test, submission)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4481,
        "step_number": 2
      },
      "source": "# 2.교육(education) 수준 카테고리 재분류\n\n데이터 전처리 과정에서는 종종 특성의 값을 재구분하는 작업이 필요합니다.   \n이는 카테고리형 변수의 값들을 더 큰 범주로 묶어 데이터의 복잡도를 줄이고,   \n모델 학습에 유용한 정보를 보다 명확하게 표현하는데 도움이 됩니다.\n\n우리의 데이터셋에서 'education' 특성은 사람들의 교육 수준을 나타내는 여러 가지 값으로 구성되어 있습니다.   \n그러나 이러한 각각의 값들이 모델 학습에 동일하게 중요하지 않을 수 있으며,   \n비슷한 교육 수준을 가진 카테고리들은 실질적으로 같은 의미를 가질 가능성이 있습니다.\n\n따라서 'education' 특성의 일부 값을 새로운 범주로 묶어보도록 하겠습니다.   \n이렇게 함으로써 우리는 데이터 내에서 비슷한 의미를 가진 여러 개의 범주를 단순화하여 모델 성능을 향상시키고자 합니다.\n\n[문제 2]\n\n- `replace()` 함수를 사용하여 초등학교 교육 수준('Preschool', '1st-4th', '5th-6th')과 중학교 초기('7th-8th') 및 후기('9th'), 고등학교 초기('10th', '11th') 및 후기('12th') 등을 포함하는 여러 개의 범주를 하나인 `LowEducation`으로 재구분해주세요.\n\n- 일부 고등 교육 수준(대학 일부 코스와 전문 대학 코스)은 `SomeHigherEd`로,    \n석사 학위와 전문 학위 프로그램은 모두 `Masters`로 재구분하였습니다."
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4481,
        "step_number": 2,
        "trusted": false
      },
      "outputs": [],
      "source": "train['education'].___(['Preschool', '10th', '11th', '12th', '1st-4th', '5th-6th', '7th-8th', '9th'], 'LowEducation', inplace=True)\ntrain['education'].replace(['Some-college', 'Assoc-acdm', 'Assoc-voc'], 'SomeHigherEd', inplace=True)\ntrain['education'].replace(['Masters', 'Prof-school'], ['Masters', 'Masters'], inplace=True)"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4481,
        "step_number": 2,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nensure_vals(globals(), 'train')\n@check_safety\ndef check(\n    df: pd.DataFrame,\n    col: str,\n    val1: str,\n    val2: str,\n    val3: str\n):\n    c_point0 = val1 in df[col].unique()\n    c_point1 = val2 in df[col].unique()\n    c_point2 = val3 not in df[col].unique()\n\n    if c_point0 and c_point1 and c_point2:\n        return True\n    else:\n        return False\n\ncheck(train, 'education', 'LowEducation', 'SomeHigherEd', 'Prof-school')"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4482,
        "step_number": 3,
        "tags": []
      },
      "source": "# 3.결혼 상태 (marital.status) 수준 카테고리 재분류\n\nNever.married는 결혼을 한 번도 하지 않은 상태를 나타내며, Married.spouse.absent는 배우자가 부재한 상태를 나타냅니다.   \n이 두 상태를 통합해주겠습니다.\n\nMarried.AF.spouse는 미국 공군의 배우자와 결혼한 상태를 나타내며, Married.civ.spouse는 미국 시민과 결혼한 상태를 나타냅니다.\n이 두 상태를 통합해주겠습니다.\n\nSeparated는 별거 중인 상태를 나타내며, Divorced는 이혼한 상태를 나타냅니다.  \n이 두 상태를 통합해주겠습니다.\n\n[문제 3]  \n\n- `train` 데이터셋의 `marital.status` 열에서 `Never.married`와 `Married.spouse.absent`의 값을 `UnmarriedStatus`로 대체해주세요.  \n`inplace=True` 매개변수를 사용하여 원본 데이터프레임을 변경하도록 설정 해주세요.\n- marital.status 열에서 `Married.AF.spouse`와 `Married.civ.spouse`의 값을 Married로 대체해주세요.\n- marital.status 열에서 `Separated와 Divorced`의 값을 `MarriageEnded`로 대체해주세요."
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4482,
        "step_number": 3,
        "trusted": false
      },
      "outputs": [],
      "source": "train['marital.status'].___(['Never.married', 'Married.spouse.absent'], 'UnmarriedStatus', inplace=___)\ntrain['marital.status'].replace(['Married.AF.spouse', 'Married.civ.spouse'], '___', inplace=True)\n___[___].___([___, ___], ___, inplace=___)"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4482,
        "step_number": 3,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nensure_vals(globals(), 'train')\n@check_safety\ndef check(\n    df: pd.DataFrame,\n    col: str,\n    val1: str,\n    val2: str,\n):\n    c_point0 = val1 in df[col].unique()\n    c_point1 = val2 not in df[col].unique()\n\n    if c_point0 and c_point1:\n        return True\n    else:\n        return False\n\ncheck(train, 'marital.status', 'MarriageEnded', 'Separated')"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4483,
        "step_number": 4
      },
      "source": "# 4.나이(age)와 주당 근무시간(hours.per.week)을 곱한 새로운 특성 생성\n\n\n데이터 분석 과정에서는 새로운 특성을 생성함으로써 모델의 성능을 향상시키는 `특성 엔지니어링(feature engineering)`이 필요합니다.   \n이는 기존 데이터에서 추가적인 정보를 추출하거나, 데이터 간의 복잡한 관계를 캡처하는 데 도움이 됩니다.\n\n우리의 경우, `age`와 `hours.per.week`라는 두 가지 특성 사이에 중요한 상호작용(interaction)이 있음을 발견하였습니다.   \n이전 단계인 stage3에서 scatterplot을 그려본 결과, 나이와 주당 근무시간 사이에 분명한 패턴 혹은 관계가 보여,   \n이 두 변수를 조합하여 새로운 특성 `age-hours`를 생성하기로 결정하였습니다.\n\n새로운 `age-hours` 특성은 개개인의 나이와 그들이 일하는 시간 사이의 복합적인 영향력을 반영할 것입니다.   \n이렇게 함으로써 우리 모델은 나이와 근무 시간 각각만 고려하는 것보다 더 많은 정보를 얻어 성능 개선에 도움을 줄 수 있게 될 것입니다."
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4483,
        "step_number": 4,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>education.num</th>\n      <th>marital.status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>capital.gain</th>\n      <th>capital.loss</th>\n      <th>hours.per.week</th>\n      <th>native.country</th>\n      <th>target</th>\n      <th>age-hours</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAIN_0000</td>\n      <td>75</td>\n      <td>Self-emp-not-inc</td>\n      <td>218521</td>\n      <td>SomeHigherEd</td>\n      <td>10</td>\n      <td>Married-spouse-absent</td>\n      <td>Craft-repair</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n      <td>United-States</td>\n      <td>0</td>\n      <td>2250</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRAIN_0001</td>\n      <td>23</td>\n      <td>Private</td>\n      <td>194102</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Never-married</td>\n      <td>Exec-managerial</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>0</td>\n      <td>920</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRAIN_0002</td>\n      <td>34</td>\n      <td>Private</td>\n      <td>238305</td>\n      <td>SomeHigherEd</td>\n      <td>10</td>\n      <td>Married-civ-spouse</td>\n      <td>Other-service</td>\n      <td>Wife</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>1628</td>\n      <td>12</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>408</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "           ID  age         workclass  fnlwgt     education  education.num  \\\n0  TRAIN_0000   75  Self-emp-not-inc  218521  SomeHigherEd             10   \n1  TRAIN_0001   23           Private  194102     Bachelors             13   \n2  TRAIN_0002   34           Private  238305  SomeHigherEd             10   \n\n          marital.status       occupation   relationship   race     sex  \\\n0  Married-spouse-absent     Craft-repair  Not-in-family  White    Male   \n1          Never-married  Exec-managerial      Unmarried  White    Male   \n2     Married-civ-spouse    Other-service           Wife  White  Female   \n\n   capital.gain  capital.loss  hours.per.week native.country  target  \\\n0             0             0              30  United-States       0   \n1             0             0              40  United-States       0   \n2             0          1628              12            NaN       0   \n\n   age-hours  \n0       2250  \n1        920  \n2        408  "
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "train['age-hours'] = train['age']*train['hours.per.week']\ntrain.head(3)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4484,
        "step_number": 5
      },
      "source": "# 5.데이터 분할 및 train/valid 데이터 준비\n\n[문제 4]\n- 독립변수 x에는 train 데이터셋에서 target,education 을 제외한 나머지 모든 열의 값들로 설정해 주세요.\n- 종속변수 y에는 train 데이터셋에서 target 열의 값으로 설정해 주세요.\n- 독립변수 x, 종속변수 y 데이터를 훈련 세트(train)와 검증 세트(valid)로 (7:3의 비율로) 나누어 보세요."
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4484,
        "step_number": 5,
        "trusted": false
      },
      "outputs": [],
      "source": "x = ...\ny = ...\n\nfrom sklearn.model_selection import train_test_split\n___, ___, ___, ___ = ___(x, y, ___ = 0.3, random_state = 42)"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4484,
        "step_number": 5,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nensure_vals(globals(), 'x_valid', 'y_valid')\n@check_safety\ndef check(\n        user_answer_x : str,\n        user_answer_y : str,\n):\n    c_point0 = hasattr(user_answer_x, 'head')\n    c_point1 = hasattr(user_answer_y, 'head')\n\n    if c_point0 and c_point1:\n        return True\n    else:\n        return False\n\ncheck(x_valid,y_valid)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4485,
        "step_number": 6
      },
      "source": "# 6.결측치 처리와 불필요한 열 제거를 위한 데이터 전처리\n\n[문제 5]\n\n- 결측치 처리를 위한 `SimpleImputer` 클래스를 가져오세요.    \n- `SimpleImputer` 객체를 생성하고, 결측치를 처리할 때 해당 특성에서 가장 자주 발생하는 값으로 결측치를 대체해주세요.\n- 학습 데이터셋인 `x_train`의 'occupation'과 'workclass' 열에 대해 결측치를 `최빈값`으로 채워주세요.\n- 검증 데이터셋인 `x_valid`의 'occupation'과 'workclass' 열에 대해 결측치를 `최빈값`으로 채워주세요.\n- x_train과 x_valid 데이터셋에서 불필요한 피처인 `'native.country','ID'` 을 제거해 주세요."
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4485,
        "step_number": 6,
        "trusted": false
      },
      "outputs": [],
      "source": "from sklearn.impute import ___\n\n# SimpleImputer를 사용하여 결측치를 최빈값으로 보간\nimputer = ___(strategy='___')\nx_train[['occupation','workclass']] = ___.___(...)\nx_valid[['occupation','workclass']] = ___.___(...)\n\n# 불필요한 열 제거\nx_train = x_train.___(...)\nx_valid = x_valid.___(...)"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4485,
        "step_number": 6,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nensure_vals(globals(), 'x_train', 'imputer')\n@check_safety\ndef check(\n    df: pd.DataFrame,\n    encoder: SimpleImputer,\n    not_col: str,\n    use_col1: str,\n    use_col2: str\n):\n    c_point0 = not_col not in df.columns\n    c_point1 = use_col1 in encoder.feature_names_in_\n    c_point1 = use_col2 in encoder.feature_names_in_\n\n    if c_point0 and c_point1:\n        return True\n    else:\n        return False\n\ncheck(x_train, imputer, 'native.country', 'workclass', 'occupation')"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4486,
        "step_number": 7
      },
      "source": "# 7.범주형 변수 원-핫 인코딩\n\n이전 스테이지에서는 모든 범주형 변수를 레이블 인코딩(label encoding)을 통해 수치형으로 변환하였습니다.   \n이 방법은 각 범주에 고유한 정수 값을 할당하여 `순서가 있는 특성(ordinal features)`을 처리하는 데 유용합니다.\n\n그러나 'race', 'sex', 'marital.status'와 같은 특성들은 `순서가 없는(nominal)` 범주형 변수입니다.   \n이런 경우, 각 범주를 독립적인 이진 특성으로 변환하는 `원-핫 인코딩(one-hot encoding)`이 더 적합합니다.   \n원-핫 인코딩은 모델에게 잘못된 정보(예: 하나의 범주가 다른 것보다 큰 것으로 해석될 수 있음)를 제공하는 것을 방지할 수 있습니다.\n\nsklearn의 `OneHotEncoder`를 사용하여 'race', 'sex', 'marital.status' 세 가지 특성에 대해 원-핫 인코딩을 적용하겠습니다.\n\n[문제 6]\n- scikit-learn의 preprocessing 모듈에서 `OneHotEncoder` 클래스를 불러오세요.\n- `OneHotEncoder` 객체를 생성합니다.   \n`sparse=False` 옵션은 희소 행렬(sparse matrix)이 아닌 밀집 행렬(dense matrix) 형태로 데이터를 반환하도록 설정합니다.\n- 학습 데이터셋(`x_train`)의 `'race', 'sex', 'marital.status'` 열을 선택하고,   \n`fit_transform` 메서드를 사용하여 원-핫 인코딩을 수행합니다.   \n이 작업은 학습 데이터에 대해 모델을 훈련하면서 해당 열의 범주를 기반으로 원-핫 인코딩을 학습합니다.\n- 검증 데이터셋(`x_valid`)에 대해 원-핫 인코딩을 수행합니다. 학습 데이터와 같은 열 순서 및 범주를 사용하여 변환합니다."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4486,
        "step_number": 7,
        "trusted": false
      },
      "outputs": [],
      "source": "from sklearn.preprocessing import ___\n\n# OneHotEncoder 객체 생성\nencoder = ___(sparse_output=False)\n\n# 학습 데이터에 대해 fit_transform 실행\nx_train_encoded = encoder.___(x_train[['race', 'sex', 'marital.status']])\n\n# 검증 데이터에 대해 transform 실행\nx_valid_encoded = encoder.transform(___[['race', 'sex', 'marital.status']])"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4486,
        "step_number": 7,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nimport numpy as np\nensure_vals(globals(), 'x_train', 'encoder', 'x_train_encoded')\n@check_safety\ndef check(\n    encoder_name: str,\n    enc: OneHotEncoder,\n    col1: str,\n    col2: str,\n    onehoted: np.ndarray,\n    indexing_col: int,\n    len_col: int\n):\n    c_point0 = encoder_name in str(enc)\n    c_point1 = col1 in enc.feature_names_in_\n    c_point2 = col2 in enc.feature_names_in_\n    c_point3 = onehoted.shape[indexing_col] == 13\n\n    if (\n        c_point0 and \n        c_point1 and\n        c_point2 and \n        c_point3\n    ):\n        return True\n    else:\n        return False\n\ncheck('OneHotEncoder', encoder, 'race', 'marital.status', x_train_encoded, 1, 13)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4487,
        "step_number": 8
      },
      "source": "# 8.원-핫 인코딩된 데이터를 DataFrame으로 변환\n\nscikit-learn의 OneHotEncoder가 반환하는 것은 numpy 배열 형태입니다.     \n이는 기계 학습 모델에 바로 입력할 수 있는 형태이지만, 우리가 데이터를 직접 다루거나 분석할 때는 pandas DataFrame 형태가 더 편리합니다.     \nDataFrame은 레이블링된 열(column)을 가지므로 어떤 열이 어떤 원래의 범주에 해당하는지 쉽게 파악할 수 있습니다.\n\n따라서 원-핫 인코딩된 numpy 배열을 pandas DataFrame으로 변환해보도록 하겠습니다. \n\n[문제 7]  \n`pd.DataFrame()` 메소드를 사용하여 원-핫 인코딩된 훈련 데이터를 새로운 데이터프레임으로 변환합니다.   \ncolumns 인자에는 `get_feature_names_out()` 메소드를 사용하여 각각의 컬럼 이름을 생성합니다."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4487,
        "step_number": 8,
        "trusted": false
      },
      "outputs": [],
      "source": "x_train_ohe = pd.___(x_train_encoded, columns=encoder.___(['race', 'sex', 'marital.status']))\nx_valid_ohe = pd.DataFrame(x_valid_encoded, columns=encoder.get_feature_names_out(['race', 'sex', 'marital.status']))\n\nx_train_ohe.head(4)"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4487,
        "step_number": 8,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nensure_vals(globals(), 'x_train_ohe')\n@check_safety\ndef check(\n    df: pd.DataFrame,\n    shape_df: tuple\n):\n    c_point0 = df.shape == shape_df\n\n    if c_point0:\n        return True\n    else:\n        return False\n\ncheck(x_train_ohe, (3108, 13))"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4488,
        "step_number": 9
      },
      "source": "# 9.데이터프레임에 원-핫 인코딩된 특성 추가\n\n데이터 처리 과정에서는 종종 여러 데이터프레임을 합치는 작업이 필요합니다.   \n이번에는 원-핫 인코딩된 데이터프레임(x_train_ohe, x_valid_ohe)과 원래의 데이터프레임(x_train, x_valid)을 합칠 것입니다.\n\npandas의 concat 함수를 사용하면 간단하게 두 데이터프레임을 합칠 수 있습니다.   \n하지만 주의할 점은, concat 함수는 기본적으로 각 데이터프레임의 인덱스를 따라 동작한다는 것입니다.   \n즉, 서로 다른 인덱스를 가진 두 데이터프레임을 합치게 되면 예상하지 못한 결과가 나올 수 있습니다.\n\n따라서 우리는 먼저 reset_index 메소드를 사용하여 x_train과 x_valid의 인덱스를 초기화하겠습니다.    \n인덱스가 초기화된 후에야 비로소 원-핫 인코딩된 피처들과 원래 피처들이 올바르게 연결시켜주겠습니다.\n\n\n\n[문제 8]\n- `reset_index(drop=True)`를 사용하여 기존의 인덱스를 삭제하고 새로운 연속적인 정수 인덱스로 설정해주세요.\n- x_train_ohe와 x_valid_ohe를 `concat()` 함수를 사용하여   \n각각의 기존 데이터프레임 x_train과 x_train_ohe을 열 방향(axis=1)으로 합쳐주세요."
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4488,
        "step_number": 9,
        "trusted": false
      },
      "outputs": [],
      "source": "x_train = x_train.___(drop=___)\nx_valid = x_valid.reset_index(drop=True)\n\nx_train = pd.___([x_train,x_train_ohe], axis=1)\nx_valid = pd.concat([x_valid,x_valid_ohe], axis=1)"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4488,
        "step_number": 9,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nensure_vals(globals(), 'x_train', 'x_valid')\n@check_safety\ndef check(\n    train_df: pd.DataFrame,\n    val_df: pd.DataFrame,\n    train_shape: tuple,\n    val_shape: tuple\n):\n    c_point0 = train_df.shape == train_shape\n    c_point1 = val_df.shape == val_shape\n\n    if c_point0 and c_point1:\n        return True\n    else:\n        return False\n\ncheck(x_train, x_valid, (3108,26), (1332,26))"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4489,
        "step_number": 10
      },
      "source": "# 10.LabelEncoder를 사용한 범주형 데이터 인코딩\n\n이전에 우리는 순서가 없는 범주형 피처들에 대해 원-핫 인코딩을 적용하였습니다.   \n그러나 아직 처리하지 않은 다른 범주형 피처들도 있습니다.   \n이들은 순서가 있는 데이터일 가능성이 있으므로, 레이블 인코딩(label encoding)을 적용하려 합니다.   \n레이블 인코딩은 각 고유한 범주값에 대해 고유한 정수값을 할당하는 방식으로, 순서 정보를 유지할 수 있게 합니다.\n\n[문제 9]   \n- `LabelEncoder` 객체를 생성하여 le 변수에 할당해 주세요.\n- `x_train` 데이터프레임의 현재 열에 LabelEncoder을 적용해 주세요.\n- 만약 검증 데이터에서 새롭게 나타나는 범주값(label)이라면, 이를 인코더의 클래스 목록(le.classes_)에 추가해 주세요.\n- `x_valid` 데이터프레임의 현재 열에 LabelEncoder을 적용해 주세요."
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4489,
        "step_number": 10,
        "trusted": false
      },
      "outputs": [],
      "source": "from sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\nfor col in x_train.columns:\n    if x_train[col].dtype == 'object':\n        \n        le = ___()\n        x_train[col] = ___.___(___[___])\n\n        for label in np.unique(x_valid[col]):\n            if label not in le.classes_:\n                le.classes_ = np.___(le.classes_, ___)\n        x_valid[col] = ___.___(___[___])"
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4489,
        "step_number": 10,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nensure_vals(globals(), 'x_train', 'x_valid')\n@check_safety\ndef check(\n    train_df: pd.DataFrame,\n    val_df: pd.DataFrame,\n    obj: str,\n    zero: 0\n):\n    \n    len_obj_train = len(train_df.select_dtypes(include=obj).columns)\n    len_obj_test = len(val_df.select_dtypes(include=obj).columns)\n    \n    c_point0 = len_obj_train == zero\n    c_point1 = len_obj_test == zero\n\n    if c_point0:\n        return True\n    else:\n        return False\n\ncheck(x_train, x_valid, 'object', 0)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4490,
        "step_number": 11
      },
      "source": "# 11.StandardScaler를 이용한 데이터 표준화\n\n데이터 전처리는 머신러닝 모델의 성능에 중요한 역할을 합니다.   \n이 과정에서 데이터의 특성들을 적절하게 변환하여 모델이 데이터를 더 잘 이해할 수 있도록 돕습니다.   \n그 중 하나가 바로 특성 스케일링입니다.\n\n특성들 간에 값의 범위가 다르면, 그 값이 큰 특성이 결과에 더 큰 영향을 주게 됩니다.   \n예를 들어, '나이'는 보통 0-100 사이의 값을 가지지만, '수입'은 수 천에서 수 만까지 다양하게 분포할 수 있습니다.   \n이런 경우 '수입' 특성이 결과에 과도하게 영향을 미치게 되므로, 모든 특성이 동등하게 반영되도록 스케일링 작업을 거치는 것입니다.\n\n[문제 10]\n- scikit-learn 라이브러리에서 제공하는 `StandardScaler` 클래스를 사용하여 scaler 객체를 생성해주세요.\n\n- 학습 데이터셋(`x_train`)에 대해서는 `fit_transform` 메서드를 사용하여 평균과 분산을 계산(fit)한 후   \n표준화(transform)를 진행합니다.\n\n- 검증 데이터셋(`x_valid`)에 대해서는 `transform` 메서드를 사용하여   \n학습 데이터셋에서 이미 계산된 평균과 분산 값을 사용하여 표준화(transform)를 진행합니다.   \n이렇게 해야 학습과 검증 단계에서 일관된 전처리가 적용됩니다."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4490,
        "step_number": 11,
        "trusted": false
      },
      "outputs": [],
      "source": "from sklearn.preprocessing import StandardScaler\n\nfeatures = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss','hours.per.week', 'age-hours']\n\nscaler = ___()\nx_train[features] = scaler.___(x_train[___])\nx_valid[features] = ___.___(x_valid[features])"
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4490,
        "step_number": 11,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nensure_vals(globals(), 'x_train', 'x_valid', 'scaler')\n@check_safety\ndef check(\n    train_df: pd.DataFrame,\n    val_df: pd.DataFrame,\n    scaler_class: StandardScaler,\n    num_features: int\n):\n    \n    c_point0 = len(scaler_class.feature_names_in_) == num_features\n\n    if c_point0:\n        return True\n    else:\n        return False\n\ncheck(x_train, x_valid, scaler, 7)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4491,
        "step_number": 12
      },
      "source": "# 12.데이터 차원 축소를 위한 PCA 모델 학습\n\n이번에는 주성분 분석(Principal Component Analysis, PCA)을 사용하여 데이터의 차원을 축소하는 작업을 수행해봅시다.   \nPCA는 다차원 데이터를 저차원 공간으로 변환하여 데이터의 변동성을 최대한 보존하는 방식으로 동작합니다.\n\n[문제 11]  \n- `Scikit-learn`의 `decomposition` 모듈에서 `PCA` 클래스를 불러와 보세요.\n- `PCA` 객체를 생성해보세요.\n- 주어진 훈련 데이터인 `x_train에` 대해 `PCA` 모델을 적용(`fit`)시켜보세요. "
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4491,
        "step_number": 12,
        "trusted": false
      },
      "outputs": [],
      "source": "from sklearn.___ import PCA\n\npca = PCA()\npca.___(x_train)"
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4491,
        "step_number": 12,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nensure_vals(globals(), 'x_train', 'pca')\n@check_safety\ndef check(\n    train_df: pd.DataFrame,\n    pca_class\n):\n    \n    c_point0 = len(train_df.columns) == len(pca_class.get_feature_names_out())\n    \n    if c_point0:\n        return True\n    else:\n        return False\n\ncheck(x_train, pca)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4492,
        "step_number": 13
      },
      "source": "# 13.PCA 주성분 추출"
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4492,
        "step_number": 13,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "array([-4.79055732e-04,  2.88123473e-03,  7.14129075e-03,  2.93943613e-02,\n       -1.19098206e-02,  9.96784763e-01, -5.38655166e-02,  5.70556703e-05,\n        1.49095433e-02,  1.10344715e-02,  2.16331617e-03,  3.14708243e-02,\n        2.42733518e-02,  6.85156857e-04,  7.17858710e-05, -1.44281378e-03,\n       -1.27413157e-04,  8.13284206e-04, -1.49095433e-02,  1.49095433e-02,\n       -1.29029930e-03, -1.05222737e-04,  7.49006552e-03, -9.83909787e-06,\n       -3.66831033e-03, -2.41639405e-03])"
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "pca.components_[0]"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4493,
        "step_number": 14
      },
      "source": "# 14.PCA 주성분의 분산 설명량 확인"
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4493,
        "step_number": 14,
        "trusted": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "explained_variance_ratio: \n\n [4.94515588e-01 1.30375995e-01 7.19915105e-02 4.47434711e-02\n\n 3.84268750e-02 3.59899981e-02 3.44973959e-02 3.34197805e-02\n\n 3.08658290e-02 3.02436508e-02 2.74980741e-02 1.31476642e-02\n\n 7.37395656e-03 2.32056637e-03 1.89438084e-03 1.12435680e-03\n\n 5.87351407e-04 5.22480355e-04 4.16154596e-04 4.49208450e-05\n\n 3.48816875e-32 1.90644454e-32 3.10229048e-33 3.10229048e-33\n\n 3.10229048e-33 3.01745573e-33]\n\n\n\nexplained_variance_ratio[0]:  0.4945155881447502\n\n\n\nexplained_variance_ratio[1]:  0.13037599498893046\n"
        }
      ],
      "source": "# 분산의 설명량 확인\nexplained_variance_ratio = pca.explained_variance_ratio_\n\nprint('explained_variance_ratio: \\n',explained_variance_ratio)\nprint('\\nexplained_variance_ratio[0]: ',explained_variance_ratio[0])\nprint('\\nexplained_variance_ratio[1]: ',explained_variance_ratio[1])"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4494,
        "step_number": 15
      },
      "source": "# 15.누적 설명량과 주성분 개수 선택\n\n주성분 분석(PCA)은 데이터의 차원을 축소하기 위한 기술 중 하나입니다.   \n보통, 축소할 차원을 임의로 정하기 보다는 충분한 분산이 될 때까지 더해야할 차원 수를 선택하는 쪽이 더 일반적입니다.   \n이 방법을 사용하면 데이터의 정보를 최대한 보존하면서도 차원을 줄일 수 있습니다.   \n\n`cumulative_explained_variance`를 계산하여 누적 설명량이 95% 이상이 되는 주성분 개수를 선택해보겠습니다.\n\n[문제 12]  \n- `cumsum` 함수를 사용하여 `explained_variance_ratio_` 값을 누적해서 더해주세요.\n- `argmax` 함수를 사용하여 해당 임계값을 넘는 최초의 인덱스를 찾아보세요."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4494,
        "step_number": 15,
        "trusted": false
      },
      "outputs": [],
      "source": "# 누적 설명량 계산\ncumulative_explained_variance = np.___(explained_variance_ratio)\n\n# 적절한 주성분 개수 선택 (예: 95% 이상의 누적 설명량을 가지는 개수)\nn_components = np.___(cumulative_explained_variance >= 0.95) + 1"
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4494,
        "step_number": 15,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nimport numpy as np\nensure_vals(globals(), 'x_train', 'cumulative_explained_variance')\n@check_safety\ndef check(\n    train_df: pd.DataFrame,\n    variance: np.ndarray\n):\n    \n    c_point0 = len(variance) == len(train_df.columns)\n    \n    if c_point0:\n        return True\n    else:\n        return False\n\ncheck(x_train, cumulative_explained_variance)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4495,
        "step_number": 16
      },
      "source": "# 16.PCA 모델 재설정 및 데이터 변환\n\n위에서 선택된 주성분 개수로 PCA 모델을 다시 초기화하고, 이전과 같은 데이터에 적용하여 차원을 축소합니다.   \n이렇게 함으로써 데이터의 차원을 축소하면서 원본 데이터의 정보를 가능한 한 보존하게 됩니다.\n\n[문제 14]  \n- 주성분 분석(PCA) 모델을 초기화해주세요.   \n이때, `n_components` 매개변수에는 이전 단계에서 선택한 주성분 개수인 `n_components`를 전달합니다.   \n\n- 초기화된 PCA 모델을 기존의 훈련 데이터인 `x_train`에 다시 적용(`fit_transform`)해주세요.   \n검증 데이터인 `x_valid`에 동일한 PCA 모델을 적용(`transform`)합니다."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4495,
        "step_number": 16,
        "trusted": false
      },
      "outputs": [],
      "source": "# 선택된 주성분 개수로 PCA 모델 다시 초기화\npca = ___(n_components=___)\n\n# 데이터에 선택된 주성분 개수로 PCA 모델 적용\nx_train = pca.___(___)\nx_valid = pca.___(___)"
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4495,
        "step_number": 16,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nimport numpy as np\nensure_vals(globals(), 'pca')\n@check_safety\ndef check(\n    train_df: pd.DataFrame,\n    pca_class: PCA\n):\n    \n    c_point0 = len(pca_class.get_feature_names_out()) == pca_class.n_components\n    \n    if c_point0:\n        return True\n    else:\n        return False\n\ncheck(x_train, pca)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pbl_cell_type": "markdown",
        "step_id": 4496,
        "step_number": 17
      },
      "source": "# 17.RandomForestClassifier 모델을 활용한 데이터 분석 및 성능 평가\n\n`x_train`과 `x_valid` 데이터셋은 이미 PCA를 적용하여 주성분으로 변환된 데이터셋입니다. \n\nPCA 후에 남아 있는 각각의 주성분들은 원래 특성 공간에서 어떠한 방향(즉, 벡터)를 나타내며,   \n이러한 방향들은 원래 데이터가 가장 많이 분산되어 있는 방향입니다.   \n따라서 PCA로 변환된 `x_train`과 `x_valid`는 원래의 다차원 공간에서 가장 정보가 많이 담긴 방향으로 투영된 상태라고 볼 수 있습니다.\n\n그런 다음 RandomForestClassifier를 사용하여 모델을 학습시킨 후  \n마지막으로 F1 score를 계산하여 모델 성능을 평가하겠습니다.\n\n\n[문제 15]\n\n- RandomForestClassifier 모델을 생성해 주세요. random_state 는 42로 설정하여 재현성을 보장해주세요.\n- 학습 데이터를 사용하여 학습시켜주세요.\n- 학습된 모델을 사용하여 검증 데이터에 대한 예측을 수행합니다.\n- f1_score 함수를 사용하여 실제 레이블인 검증 데이터의 목표값과 모델의 예측 결과를 비교하여 Macro F1 스코어를 계산해보세요. "
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {
        "pbl_cell_type": "code",
        "step_id": 4496,
        "step_number": 17,
        "trusted": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Macro F1 스코어: 0.7929340560259506\n"
        }
      ],
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\n\nrf_classifier = ___(random_state=___)\n___.fit(___, ___)\n\ny_pred = ___.___(x_valid)\n\nmacro_f1 = f1_score(y_valid, y_pred, average='___')\nprint(\"Macro F1 스코어:\", macro_f1)"
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "pbl_cell_type": "hidden_checkcode",
        "step_id": 4496,
        "step_number": 17,
        "trusted": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 181,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "#checkcode\nimport numpy as np\nensure_vals(globals(), 'x_train', 'y_train', 'rf_classifier', 'y_pred', 'macro_f1')\n@check_safety\ndef check(\n    x_train_df: pd.DataFrame,\n    y_train_df: pd.DataFrame,\n    x_valid_df: pd.DataFrame,\n    val_pred: np.ndarray,\n    model: RandomForestClassifier,\n    model_name: str\n):\n    \n    c_point0 = model_name in str(model)\n    c_point1 = len(model.classes_) == y_train_df.nunique()\n    c_point2 = len(val_pred) == len(x_valid_df)\n    \n    if c_point0:\n        return True\n    else:\n        return False\n\ncheck(x_train, y_train, x_valid, y_pred, rf_classifier, 'RandomForestClassifier')"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}